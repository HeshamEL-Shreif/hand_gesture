# Hand Gesture Classification using HaGRID and MediaPipe

## Overview
This project focuses on classifying hand gestures using landmark data generated by MediaPipe from the HaGRID dataset. The dataset consists of 18 different hand gestures, and the goal is to train machine learning models to recognize these gestures accurately.

## Dataset
- The dataset contains hand landmark data extracted using MediaPipe.
- Each hand has 21 landmarks, each with (x, y, z) coordinates.
- The dataset is stored in a CSV file where each row corresponds to a hand gesture with its label.

## Preprocessing Steps
- **Re-centering Landmarks:** The hand landmarks are translated such that the wrist becomes the origin.
- **Normalization:** The coordinates are divided by the mid-finger tip position to standardize scale across images.
- **Feature Engineering:** The landmark coordinates are flattened into a single feature vector per sample.
- **Train-Test Split:** The data is split into training and testing sets with stratification to maintain class distribution.

## Models Used
- Several models were trained and evaluated, including:
  - **Support Vector Machine (SVM)**
  - **Random Forest** (Best-performing model)
  - **K-Nearest Neighbors (KNN)**
  - **Decision Tree**
  - **Logistic regression**
  - **GradientBoostingClassifier**
- Each trained model has been saved as a `.pkl` file in the repository.

## Results & Conclusion
- The **Random Forest** classifier achieved the best performance among all models.
- The normalization technique (centering at the wrist and dividing by the mid-finger tip) improved accuracy by making hand representations more consistent.
- Future improvements could include deep learning approaches for further accuracy enhancement.

## Repository Contents
- `hand_landmark_data.csv`: dataset
- `random_forest.pkl`, `svm.pkl`, `knn.pkl`, etc.: Trained model files
- `hand_gesture.py`: project script
- `preprocess_landmark.py`: landmark preprocessing

## How to Run
1. Install dependencies: `pip install -r requirements.txt`
2. Run preprocessing: `hand_gesture.py`


## Acknowledgments
- HaGRID Dataset
- MediaPipe for Hand Landmark Detection

